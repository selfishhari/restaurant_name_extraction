{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b5ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('restaurants_train.csv')\n",
    "val = pd.read_csv('restaurants_val.csv')\n",
    "holdout = pd.read_csv('restaurants_holdout.csv')\n",
    "# train = train.fillna('')\n",
    "# val = val.fillna('')\n",
    "# holdout = holdout.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95eeaaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.restaurant_name.isna().sum()/train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134a7844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.restaurant_name.isna().sum()/val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd25408c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout.restaurant_name.isna().sum()/holdout.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d05e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rn = train.restaurant_name.unique().tolist()\n",
    "\n",
    "val_rn = val.restaurant_name.unique().tolist()\n",
    "\n",
    "holdout_rn = holdout.restaurant_name.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e650536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 11\n",
      "41 45\n"
     ]
    }
   ],
   "source": [
    "print(len(set(val_rn) - set(train_rn)), len(val_rn))\n",
    "\n",
    "print(len(set(holdout_rn) - set(train_rn)), len(holdout_rn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d560780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3322e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna('')\n",
    "val = val.fillna('')\n",
    "holdout = holdout.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f007a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tag_bio(txt, entity):    \n",
    "        \"\"\"\n",
    "        Given a text and an entity, creates BIO tags for that entity.\n",
    "        \"\"\"\n",
    "\n",
    "        entity_tok = entity.split()\n",
    "\n",
    "        txt_tok = txt.split()\n",
    "\n",
    "        out_tags = [\"O\"]*len(txt_tok)\n",
    "        \n",
    "        if entity == \"\":\n",
    "            return out_tags\n",
    "\n",
    "        b_idxs = []\n",
    "\n",
    "        # Fetch indices of B\n",
    "        for i, w in enumerate(txt_tok):\n",
    "            if w == entity_tok[0]:\n",
    "                b_idxs.append(i)\n",
    "\n",
    "        #print(b_idxs, txt_tok)\n",
    "\n",
    "        # if entity is just of 1 word, then tag Bs appropriately and exit\n",
    "        if len(entity_tok) == 1:\n",
    "            for b in b_idxs:\n",
    "                out_tags[b] = \"B\"\n",
    "            return out_tags\n",
    "\n",
    "        # ------- Tagging Is-----#\n",
    "        tags_idx = {}\n",
    "        for bidx in b_idxs:\n",
    "            # for each B as pivot, fetch it's corresponding Is\n",
    "            if (bidx < (len(txt_tok) - 1)):\n",
    "\n",
    "                counter = 0\n",
    "\n",
    "                while counter < (len(entity_tok) - 1):\n",
    "                    # loop till all words in entity have been iterated\n",
    "                    counter += 1\n",
    "\n",
    "                    if (bidx + counter > (len(txt_tok) - 1)):\n",
    "                        # exit if reached end of sentence before looping through all of entity words\n",
    "                        counter -= 1\n",
    "                        break\n",
    "                    if (txt_tok[bidx + counter] != entity_tok[counter]):\n",
    "                        # exit if any I word doesnt match\n",
    "                        counter -= 1\n",
    "                        # check for matching Is\n",
    "                        break;\n",
    "                if counter == (len(entity_tok) - 1):\n",
    "                    # if all I words match then counter should be equal to number of Is\n",
    "                    # if so, then add the indexes of Is to its corresponding B dict\n",
    "                    tags_idx[bidx] = [bidx + i for i in range(len(entity_tok))]\n",
    "\n",
    "        #print(tags_idx)\n",
    "        # now for every B-I index key val pairs, get BI tags\n",
    "        for b, i in tags_idx.items():\n",
    "            if len(i) > 1:\n",
    "                out_tags[b] = \"B\"\n",
    "                for ii in i[1:]:\n",
    "                    out_tags[ii] = \"I\"\n",
    "\n",
    "        return out_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "665401fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a four star restaurant with a bar</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>areas that allow smoking</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>any restaurants that still allow smoking</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are there any restaurants for diabetics that s...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can you find east dedham pizzeria that have a ...</td>\n",
       "      <td>east</td>\n",
       "      <td>[O, O, O, B, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>any mexican places have a tameles special today</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>do you know if elmos have a dress code</td>\n",
       "      <td>elmos</td>\n",
       "      <td>[O, O, O, O, B, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>are there any chicken wing places nearby</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>are there any vietnamese restaurants nearby</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>are reservations available for four people for...</td>\n",
       "      <td>112 eatery</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B, I]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence restaurant_name  \\\n",
       "0                    a four star restaurant with a bar                   \n",
       "1                             areas that allow smoking                   \n",
       "2             any restaurants that still allow smoking                   \n",
       "3    are there any restaurants for diabetics that s...                   \n",
       "4    can you find east dedham pizzeria that have a ...            east   \n",
       "..                                                 ...             ...   \n",
       "115    any mexican places have a tameles special today                   \n",
       "116             do you know if elmos have a dress code           elmos   \n",
       "117           are there any chicken wing places nearby                   \n",
       "118        are there any vietnamese restaurants nearby                   \n",
       "119  are reservations available for four people for...      112 eatery   \n",
       "\n",
       "                                        tags  \n",
       "0                      [O, O, O, O, O, O, O]  \n",
       "1                               [O, O, O, O]  \n",
       "2                         [O, O, O, O, O, O]  \n",
       "3          [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4    [O, O, O, B, O, O, O, O, O, O, O, O, O]  \n",
       "..                                       ...  \n",
       "115                 [O, O, O, O, O, O, O, O]  \n",
       "116              [O, O, O, O, B, O, O, O, O]  \n",
       "117                    [O, O, O, O, O, O, O]  \n",
       "118                       [O, O, O, O, O, O]  \n",
       "119  [O, O, O, O, O, O, O, O, O, O, O, B, I]  \n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"tags\"] = train.apply(lambda x: _tag_bio(x[\"sentence\"], x[\"restaurant_name\"]), axis=1)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c73d234f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>are there any ice cream shops in my neighborho...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>are there any restaurants within 5 miles that ...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>are there any locally owned franchises that gi...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are there any restaurants that will let me tak...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are there any five star restaurants around here</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>do you think the noodle bar is open</td>\n",
       "      <td>noodle bar</td>\n",
       "      <td>[O, O, O, O, B, I, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>are there any vegetarian restaurants in this town</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>any places around here that has a nice view</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>are there any jazz clubs that serve food</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>do any famous people frequent the jimmys pizza...</td>\n",
       "      <td>jimmys pizza</td>\n",
       "      <td>[O, O, O, O, O, O, B, I, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>are there any dining specials at le bec fin</td>\n",
       "      <td>le bec fin</td>\n",
       "      <td>[O, O, O, O, O, O, B, I, I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>are there any seafood restaurants near governm...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>are there any places near by that sell hamburg...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>are there any restaurants nearby that have out...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>can you find the waterfront restaurant alberto...</td>\n",
       "      <td>albertos deli</td>\n",
       "      <td>[O, O, O, O, O, O, B, I, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>do you know if reggianos serve breakfast</td>\n",
       "      <td>reggianos</td>\n",
       "      <td>[O, O, O, O, B, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>are there any vegetarian restaurants that allo...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>can you make reservations for two at heartland...</td>\n",
       "      <td>heartland restaurant</td>\n",
       "      <td>[O, O, O, O, O, O, O, B, I, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>are there any greek restaurants in the area</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>are there any vegan spots that are open after ...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>can you find the closest ihop</td>\n",
       "      <td>ihop</td>\n",
       "      <td>[O, O, O, O, O, B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>call cheeseboard in berkeley for me</td>\n",
       "      <td>cheeseboard</td>\n",
       "      <td>[O, B, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>does burger king accept credit cards</td>\n",
       "      <td>burger king</td>\n",
       "      <td>[O, B, I, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>beer and hot wings in town</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>are there any restaurants with valet parking a...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>any bbq places open before 5 nearby</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>are there are any cracker barrells on long island</td>\n",
       "      <td>cracker barrells</td>\n",
       "      <td>[O, O, O, O, B, I, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>best chinese food in the area</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>any good vegan spots nearby</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>are there any places near by that serve lunch ...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence       restaurant_name  \\\n",
       "0   are there any ice cream shops in my neighborho...                         \n",
       "1   are there any restaurants within 5 miles that ...                         \n",
       "2   are there any locally owned franchises that gi...                         \n",
       "3   are there any restaurants that will let me tak...                         \n",
       "4     are there any five star restaurants around here                         \n",
       "5                 do you think the noodle bar is open            noodle bar   \n",
       "6   are there any vegetarian restaurants in this town                         \n",
       "7         any places around here that has a nice view                         \n",
       "8            are there any jazz clubs that serve food                         \n",
       "9   do any famous people frequent the jimmys pizza...          jimmys pizza   \n",
       "10        are there any dining specials at le bec fin            le bec fin   \n",
       "11  are there any seafood restaurants near governm...                         \n",
       "12  are there any places near by that sell hamburg...                         \n",
       "13  are there any restaurants nearby that have out...                         \n",
       "14  can you find the waterfront restaurant alberto...         albertos deli   \n",
       "15           do you know if reggianos serve breakfast             reggianos   \n",
       "16  are there any vegetarian restaurants that allo...                         \n",
       "17  can you make reservations for two at heartland...  heartland restaurant   \n",
       "18        are there any greek restaurants in the area                         \n",
       "19  are there any vegan spots that are open after ...                         \n",
       "20                      can you find the closest ihop                  ihop   \n",
       "21                call cheeseboard in berkeley for me           cheeseboard   \n",
       "22               does burger king accept credit cards           burger king   \n",
       "23                         beer and hot wings in town                         \n",
       "24  are there any restaurants with valet parking a...                         \n",
       "25                any bbq places open before 5 nearby                         \n",
       "26  are there are any cracker barrells on long island      cracker barrells   \n",
       "27                      best chinese food in the area                         \n",
       "28                        any good vegan spots nearby                         \n",
       "29  are there any places near by that serve lunch ...                         \n",
       "\n",
       "                                             tags  \n",
       "0      [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1               [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "2               [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4                        [O, O, O, O, O, O, O, O]  \n",
       "5                        [O, O, O, O, B, I, O, O]  \n",
       "6                        [O, O, O, O, O, O, O, O]  \n",
       "7                     [O, O, O, O, O, O, O, O, O]  \n",
       "8                        [O, O, O, O, O, O, O, O]  \n",
       "9         [O, O, O, O, O, O, B, I, O, O, O, O, O]  \n",
       "10                    [O, O, O, O, O, O, B, I, I]  \n",
       "11     [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "12              [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "13                    [O, O, O, O, O, O, O, O, O]  \n",
       "14  [O, O, O, O, O, O, B, I, O, O, O, O, O, O, O]  \n",
       "15                          [O, O, O, O, B, O, O]  \n",
       "16     [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "17     [O, O, O, O, O, O, O, B, I, O, O, O, O, O]  \n",
       "18                       [O, O, O, O, O, O, O, O]  \n",
       "19           [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "20                             [O, O, O, O, O, B]  \n",
       "21                             [O, B, O, O, O, O]  \n",
       "22                             [O, B, I, O, O, O]  \n",
       "23                             [O, O, O, O, O, O]  \n",
       "24        [O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "25                          [O, O, O, O, O, O, O]  \n",
       "26                    [O, O, O, O, B, I, O, O, O]  \n",
       "27                             [O, O, O, O, O, O]  \n",
       "28                                [O, O, O, O, O]  \n",
       "29              [O, O, O, O, O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val[\"tags\"] = val.apply(lambda x: _tag_bio(x[\"sentence\"], x[\"restaurant_name\"]), axis=1)\n",
    "\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dceb8af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>find pizza places</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>find me the best rated chinese restaurant in t...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what kind of food does abc cafe serve</td>\n",
       "      <td>abc cafe</td>\n",
       "      <td>[O, O, O, O, O, B, I, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how far away is the nearest steak house</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am looking for a mexican restuarant that has...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>find me brazilian food with on location parking</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>get me to a mexican place</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>how far am i from the nearest bagel shop</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>what time does sonic open</td>\n",
       "      <td>sonic</td>\n",
       "      <td>[O, O, O, B, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>give me a list of restaurants that have seafoo...</td>\n",
       "      <td></td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence restaurant_name  \\\n",
       "0                                    find pizza places                   \n",
       "1    find me the best rated chinese restaurant in t...                   \n",
       "2                what kind of food does abc cafe serve        abc cafe   \n",
       "3              how far away is the nearest steak house                   \n",
       "4    i am looking for a mexican restuarant that has...                   \n",
       "..                                                 ...             ...   \n",
       "145    find me brazilian food with on location parking                   \n",
       "146                          get me to a mexican place                   \n",
       "147           how far am i from the nearest bagel shop                   \n",
       "148                          what time does sonic open           sonic   \n",
       "149  give me a list of restaurants that have seafoo...                   \n",
       "\n",
       "                                     tags  \n",
       "0                               [O, O, O]  \n",
       "1       [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "2                [O, O, O, O, O, B, I, O]  \n",
       "3                [O, O, O, O, O, O, O, O]  \n",
       "4    [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "..                                    ...  \n",
       "145              [O, O, O, O, O, O, O, O]  \n",
       "146                    [O, O, O, O, O, O]  \n",
       "147           [O, O, O, O, O, O, O, O, O]  \n",
       "148                       [O, O, O, B, O]  \n",
       "149  [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout[\"tags\"] = holdout.apply(lambda x: _tag_bio(x[\"sentence\"], x[\"restaurant_name\"]), axis=1)\n",
    "\n",
    "holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd658945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-13 22:23:01.562863: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-13 22:23:01.562890: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu102'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import os, sys\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aedac79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/media/narahari/DATA/Personal/hackathons/qbe/candidate_test/bert_base_uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb6c632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tags.apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9055621",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = train[\"sentence\"].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "train_labels = train[\"tags\"].tolist()\n",
    "\n",
    "val_sents = val[\"sentence\"].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "val_labels = val[\"tags\"].tolist()\n",
    "\n",
    "holdout_sents = holdout[\"sentence\"].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "holdout_labels = holdout[\"tags\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e058f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "bs = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('/media/narahari/DATA/Personal/hackathons/qbe/candidate_test/bert_base_uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ad13c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "100e35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(train_sents, train_labels)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ac8e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(val_sents, val_labels)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ed9eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(holdout_sents, holdout_labels)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d67bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized_texts = [token_label_pair[0] for token_label_pair in train_tokenized_texts_and_labels]\n",
    "\n",
    "train_labels = [token_label_pair[1] for token_label_pair in train_tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8182ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokenized_texts = [token_label_pair[0] for token_label_pair in val_tokenized_texts_and_labels]\n",
    "\n",
    "val_labels = [token_label_pair[1] for token_label_pair in val_tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5735156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_tokenized_texts = [token_label_pair[0] for token_label_pair in holdout_tokenized_texts_and_labels]\n",
    "\n",
    "holdout_labels = [token_label_pair[1] for token_label_pair in holdout_tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af51c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in train_tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6665bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_val = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in val_tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9ee8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_holdout = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in holdout_tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14181f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': 0, 'I': 1, 'O': 2, 'PAD': 3}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values = [\"B\", \"I\", \"O\", \"PAD\"]\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "327f5a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in train_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18f392b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in val_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1657e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in holdout_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f2be231",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dcffaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28b5d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids_holdout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c51530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(input_ids_train)\n",
    "\n",
    "val_inputs = torch.tensor(input_ids_val)\n",
    "\n",
    "holdout_inputs = torch.tensor(input_ids_holdout)\n",
    "\n",
    "train_tags = torch.tensor(train_tags)\n",
    "\n",
    "val_tags = torch.tensor(val_tags)\n",
    "\n",
    "holdout_tags = torch.tensor(holdout_tags)\n",
    "\n",
    "train_attention_masks = torch.tensor(train_attention_masks)\n",
    "\n",
    "val_attention_masks = torch.tensor(val_attention_masks)\n",
    "\n",
    "holdout_attention_masks = torch.tensor(holdout_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f20a0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 50]), torch.Size([30, 50]), torch.Size([30, 50]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs.shape, val_tags.shape, val_attention_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fef95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_attention_masks, train_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_attention_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0737cef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.7.0'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e02b6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/narahari/DATA/Personal/hackathons/qbe/candidate_test/bert_base_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /media/narahari/DATA/Personal/hackathons/qbe/candidate_test/bert_base_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"/media/narahari/DATA/Personal/hackathons/qbe/candidate_test/bert_base_uncased\",\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n",
    "#model.cuda();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b97e1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f978558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 600\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "print(num_warmup_steps, total_steps)\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0dfc9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# DO NOT CHANGE THIS FUNCTION! #\n",
    "################################\n",
    "\n",
    "def get_f1_score_on_test_data(model, data):\n",
    "    model.eval()\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for index, row in data.iterrows():\n",
    "        sentence = row.sentence\n",
    "        expected = row.restaurant_name\n",
    "        #inputs = tokenizer([sentence], max_length=1024, return_tensors='pt')\n",
    "        predicted = generate_outputs(model, sentence)\n",
    "        if expected != '' and expected == predicted:\n",
    "            true_positives += 1\n",
    "        if expected != '' and expected != predicted:\n",
    "            false_positives += 1\n",
    "        if expected == '' and predicted != '':\n",
    "            false_positives += 1\n",
    "        if expected != '' and predicted == '':\n",
    "            false_negatives += 1\n",
    "\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1_score = 0\n",
    "    if true_positives + false_positives:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    if true_positives + false_negatives:\n",
    "        recall = true_positives /(true_positives + false_negatives)\n",
    "    if precision + recall:\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(f'precision: {precision} | recall {recall} | f1_score {f1_score}')\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def convert_bio_text(words, tags):\n",
    "    \n",
    "    prev_tag = \"O\"\n",
    "    extracted_dict = {}\n",
    "    count = 0\n",
    "    for x,t in zip(words, tags):\n",
    "        if prev_tag == \"O\":\n",
    "            if t == \"B\":\n",
    "                prev_tag = \"B\"\n",
    "                extracted_dict[count] = [x]\n",
    "        else:\n",
    "            if t == \"I\":\n",
    "                extracted_dict[count].append(x)\n",
    "                prev_tag = \"I\"\n",
    "            elif t == \"B\" :\n",
    "                prev_tag = \"B\"\n",
    "                count += 1\n",
    "                extracted_dict[count] = [x]\n",
    "            else:\n",
    "                count += 1\n",
    "                prev_tag = \"O\"\n",
    "                \n",
    "    for i in extracted_dict.keys():\n",
    "        extracted_dict[i] = \" \".join(extracted_dict[i])\n",
    "    return list(extracted_dict.values())\n",
    "\n",
    "def generate_outputs(model, sentence, device=\"cpu\"):\n",
    "    \n",
    "    tokenized_sentence = tokenizer.encode(sentence)\n",
    "    \n",
    "    input_ids = torch.tensor([tokenized_sentence]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "        \n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "\n",
    "    new_tokens, new_labels = [], []\n",
    "\n",
    "    for token, label_idx in zip(tokens, label_indices[0]):\n",
    "\n",
    "        if token.startswith(\"##\"):\n",
    "\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "\n",
    "        else:\n",
    "\n",
    "            new_labels.append(tag_values[label_idx])\n",
    "\n",
    "            new_tokens.append(token)\n",
    "            \n",
    "    return \" \".join(convert_bio_text(new_tokens, new_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3970df83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:15<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.6317247113212943\n",
      "Validation loss: 0.4556127429008484\n",
      "val on val\n",
      "precision: 0.0 | recall 0.0 | f1_score 0\n",
      "val on holdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|███▋                                 | 1/10 [01:26<12:59, 86.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.0 | recall 0.0 | f1_score 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:13<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.37825265315671763\n",
      "Validation loss: 0.42789586358703674\n",
      "val on val\n",
      "precision: 0.0 | recall 0.0 | f1_score 0\n",
      "val on holdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|███████▍                             | 2/10 [02:50<11:20, 85.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.0 | recall 0.0 | f1_score 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:14<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.2973119808981816\n",
      "Validation loss: 0.3812241239240393\n",
      "val on val\n",
      "precision: 0.0 | recall 0.0 | f1_score 0\n",
      "val on holdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███████████                          | 3/10 [04:16<09:58, 85.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.0 | recall 0.0 | f1_score 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:19<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.26905825187956606\n",
      "Validation loss: 0.1846243464271538\n",
      "val on val\n",
      "precision: 0.0 | recall 0.0 | f1_score 0\n",
      "val on holdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|██████████████▊                      | 4/10 [05:47<08:45, 87.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.02 | recall 0.02 | f1_score 0.02\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:17<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.19417189069984792\n",
      "Validation loss: 0.12326045775941262\n",
      "val on val\n",
      "precision: 0.3 | recall 0.3 | f1_score 0.3\n",
      "val on holdout\n",
      "precision: 0.14285714285714285 | recall 0.17391304347826086 | f1_score 0.1568627450980392\n",
      "saving model: 0.1568627450980392 0.3 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|██████████████████▌                  | 5/10 [07:25<07:36, 91.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.36585365853658536 | recall 0.4166666666666667 | f1_score 0.38961038961038963\n",
      "Train f1 0.38961038961038963\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:17<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.16141979740544532\n",
      "Validation loss: 0.10396666390588508\n",
      "val on val\n",
      "precision: 0.3 | recall 0.3333333333333333 | f1_score 0.3157894736842105\n",
      "val on holdout\n",
      "precision: 0.17543859649122806 | recall 0.23809523809523808 | f1_score 0.20202020202020202\n",
      "saving model: 0.20202020202020202 0.3157894736842105 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████████████████████▏              | 6/10 [09:04<06:15, 93.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.5121951219512195 | recall 0.5526315789473685 | f1_score 0.5316455696202531\n",
      "Train f1 0.5316455696202531\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:14<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.17096935834172958\n",
      "Validation loss: 0.10198502220252219\n",
      "val on val\n",
      "precision: 0.3 | recall 0.3333333333333333 | f1_score 0.3157894736842105\n",
      "val on holdout\n",
      "precision: 0.2537313432835821 | recall 0.4594594594594595 | f1_score 0.3269230769230769\n",
      "saving model: 0.3269230769230769 0.3157894736842105 0.3157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|█████████████████████████▉           | 7/10 [10:38<04:42, 94.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.5227272727272727 | recall 0.6388888888888888 | f1_score 0.575\n",
      "Train f1 0.575\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:15<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.1284369447976739\n",
      "Validation loss: 0.10059261045147044\n",
      "val on val\n",
      "precision: 0.4 | recall 0.4444444444444444 | f1_score 0.4210526315789474\n",
      "val on holdout\n",
      "precision: 0.29333333333333333 | recall 0.5945945945945946 | f1_score 0.3928571428571429\n",
      "saving model: 0.3928571428571429 0.4210526315789474 0.3157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|█████████████████████████████▌       | 8/10 [12:14<03:09, 94.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.5909090909090909 | recall 0.7027027027027027 | f1_score 0.6419753086419754\n",
      "Train f1 0.6419753086419754\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:14<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.10282608607085422\n",
      "Validation loss: 0.10074510939108829\n",
      "val on val\n",
      "precision: 0.6 | recall 0.6666666666666666 | f1_score 0.631578947368421\n",
      "val on holdout\n",
      "precision: 0.3333333333333333 | recall 0.65 | f1_score 0.4406779661016949\n",
      "saving model: 0.4406779661016949 0.631578947368421 0.4210526315789474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████████████████████████████▎   | 9/10 [13:49<01:34, 94.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6444444444444445 | recall 0.7837837837837838 | f1_score 0.7073170731707318\n",
      "Train f1 0.7073170731707318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:14<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.10769669802878828\n",
      "Validation loss: 0.10247963322229528\n",
      "val on val\n",
      "precision: 0.6 | recall 0.6666666666666666 | f1_score 0.631578947368421\n",
      "val on holdout\n",
      "precision: 0.3333333333333333 | recall 0.65 | f1_score 0.4406779661016949\n",
      "saving model: 0.4406779661016949 0.631578947368421 0.631578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|████████████████████████████████████| 10/10 [15:24<00:00, 92.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6444444444444445 | recall 0.7837837837837838 | f1_score 0.7073170731707318\n",
      "Train f1 0.7073170731707318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Store the average loss after each epoch so we can plot them.\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "MODEL_PATH = \"bert_dropouts_30pc\"\n",
    "\n",
    "max_f1 = 0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, total = len(train_dataloader), position=0, leave=True)):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    \n",
    "    print(\"val on val\")\n",
    "    f1_val = get_f1_score_on_test_data(model, val)\n",
    "    print(\"val on holdout\")\n",
    "    f1 = get_f1_score_on_test_data(model, holdout)\n",
    "    if (f1_val >= max_f1) & (f1_val > 0):\n",
    "\n",
    "        #f1 = get_f1_score_on_test_data(model, holdout)        \n",
    "        print(\"saving model:\", f1, f1_val,max_f1)\n",
    "        max_f1 = f1_val\n",
    "        holdout_f1 = f1\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        f1_train = get_f1_score_on_test_data(model, train)\n",
    "        print(\"Train f1\", f1_train)\n",
    "        \n",
    "#     print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "#     print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a177f718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4406779661016949"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cbae17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6444444444444445 | recall 0.7837837837837838 | f1_score 0.7073170731707318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7073170731707318"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_f1_score_on_test_data(model, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "426f5682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.3333333333333333 | recall 0.65 | f1_score 0.4406779661016949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4406779661016949"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_f1_score_on_test_data(model, holdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
